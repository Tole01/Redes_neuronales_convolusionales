{
 "cells": [
  {
   "cell_type": "raw",
   "id": "203bed49",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-04-29T22:17:50.577417Z",
     "iopub.status.busy": "2023-04-29T22:17:50.576827Z",
     "iopub.status.idle": "2023-04-29T22:17:50.605985Z",
     "shell.execute_reply": "2023-04-29T22:17:50.605028Z",
     "shell.execute_reply.started": "2023-04-29T22:17:50.577373Z"
    },
    "papermill": {
     "duration": 0.007474,
     "end_time": "2023-05-12T06:34:26.155788",
     "exception": false,
     "start_time": "2023-05-12T06:34:26.148314",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe560b0",
   "metadata": {
    "papermill": {
     "duration": 0.005888,
     "end_time": "2023-05-12T06:34:26.168133",
     "exception": false,
     "start_time": "2023-05-12T06:34:26.162245",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Redes Convolucionales\n",
    "\n",
    "Una red convolucional es una representación de la visión humana, la cual usa filtros para extraer la información relevante de una imagen para clasificarla en cierta categoría.\n",
    "\n",
    "# Funcionamiento\n",
    "\n",
    "Una convolución es un filtro que pasa sobre una imagen desde la esquina superior izquierda hasta la esquina inferior derecha. Este filtro procesa uno a uno c/pixel de la imagen y extrae las características que muestran que algo es común en la imagen. Esta tecnica analiza las imágenes desde sus bordes, características, forma, etc.\n",
    "\n",
    "Conceptos clave de este sistema son los hiperparametros definidos por:\n",
    "\n",
    "* Kernel \n",
    "* Padding \n",
    "* Strides\n",
    "\n",
    "El Kernel Size, se refiere a las dimensiones del filtro, que generalmente es cuadrado, por lo que solo se indica una de las dimensiones.\n",
    "\n",
    "El Padding es el tamaño del marco (en pixeles adicionales) que se agrega a la imagen, de tal forma que los pixeles de las esquinas y bordes de la imagen original, no se pierdan del todo con la convolución.\n",
    "\n",
    "Strides es el tamaño del paso en cada convolución. Entre mas grande sea el paso menor será la imagen resultante.\n",
    "\n",
    "# Manejo de Imagenes\n",
    "\n",
    "Si no se requieren los colores para identificar algo, no los uses, ya que la escala de grises solo maneja 1 escala dentro de la dimension extra, pero las fotos y los videos a color mantienen 3 debido al RGB, por lo que el tiempo de procesamiento se triplica.\n",
    "\n",
    "De igual forma, debemos entrenar, validar y predecir con imagenes del mismo tamaño. Si son imagenes de 28 x 28 en el entrenamiento, entonces el modelo solo aceptara estas imagenes. En caso contrario podria dar error al intentar compilar, ya que el algoritmo no entiende esto.\n",
    "\n",
    "Podemos hacer uso de la tecnica de data augmentation para generar imagenes desde diferentes perspectivas, para que el modelo entienda la clasificacion.\n",
    "\n",
    "# Kernell\n",
    "\n",
    "Las redes neuronales tienen una componente principal conocida como filtros, también llamados kernel. Esto se debe a que las maquinas usualmente entienden las imágenes como pixeles, números contenidos en un sistema matricial en su ancho y su largo.\n",
    "\n",
    "El **Kernell** es simplemente una matriz que funciona de la siguiente manera:\n",
    "\n",
    "El **Kernell** se va mueve desde la esquina superior izquierda hasta la esquina inferior derecha, paso por paso hasta completa toda la imagen haciendo una pequeña operación matemática llamada “**convolución**”.\n",
    "\n",
    "La **convolución** es muy parecida al procedimiento del producto punto, pero no igual. Primero, el Kernell se ubica en la parte superior izquierda de la imagen y se va moviendo paso a paso ejecutando una operación matemática de multiplicación para empezar a obtener datos y patrones por cada fila y columna que compone la imagen. En este caso, la multiplicacion se hace entre los puntos equivalentes.\n",
    "\n",
    "Ejemplo, si tenemos una imagen de m x n, y un kernel de 2x2, nuestro kernell se ira moviendo de izquierda a derecha para multiplicar los numeros correspondientes a su posicion, entonces, tendriamos lo siguiente:\n",
    "\n",
    "$\\begin{equation} \\begin{bmatrix} 0,0 & 0,1 \\\\ 1,0 & 1,1 \\end{bmatrix} \\end{equation} x$ $\\begin{equation} \\begin{bmatrix} 0,0 & 0,1\\\\ 1,0 & 1,1 \\end{bmatrix} \\end{equation} =$ $\\begin{equation} \\begin{bmatrix} 0,0 x 0,0 & 0,1 x 0,1\\\\ 1,0 x 1,0 & 1,1 x 1,1 \\end{bmatrix} \\end{equation} = $ $\\begin{equation} \\begin{bmatrix} num1 & num2\\\\ num3 & num4 \\end{bmatrix} \\end{equation}$\n",
    "\n",
    "Finalmente, todos los numeros de la matriz se sumaran para generar un valor unico:\n",
    "\n",
    "$num1 + num2 + num3 + num4 = y$\n",
    "\n",
    "si el numero es 0, entonces significa que no hay ningun borde. Entre mayor sea y, el borde sera muchisimo mas definido.\n",
    "\n",
    "Los diferentes resultados que tenga y a lo largo de la imagen compondrán una nueva imagen con ciertas características, como por ejemplo: “los bordes verticales”. El objetivo de la convolución es empezar a obtener datos y patrones que representen todos los bordes posibles en distintas orientaciones, ya que hay 1 kernell para cada orientacion.\n",
    "\n",
    "El ejemplo practico de esta teoria se le puede encontrar en https://setosa.io/ev/image-kernels/ o en mi notebook de Procesamiento de Imagen.\n",
    "\n",
    "# Padding\n",
    "\n",
    "Es un margen que se le agrega a la imagen para que al momento de realizar la operación de convolución la imagen resultante no reduzca su tamaño. Se utilizan 0 para que no altere las características de las imágenes original. Este es util para brindarle valores de referencia a los pixeles que se encuentran en el perimetro de la imagen.\n",
    "\n",
    "# Strides\n",
    "\n",
    "Es el tamaño del paso en cada convolución y entre más grande sea el paso menor será la imagen resultante. Este puede simplificarse como el step que da el kernel a la hora de procesar la imagen.\n",
    "\n",
    "# Hiperparametros en la capa de convolucion\n",
    "\n",
    "Al invocar la capa de convolución `Conv2D()`, nosotros podemos modificar los hiperparametros a traves de los siguientes argumentos:\n",
    "\n",
    "* Filters: cuantos Kernel’s voy a utilizar.\n",
    "* Kernel_size: el tamaño del Kernel.\n",
    "* Strides: que se reciben como una tupla en la cual el primer parámetro registra el movimiento hacia los lados y el segundo, su movimiento hacia arriba o hacia abajo. Ejemplo: (1,1).\n",
    "* Padding: recibe en un array dos valores `“valid”` y `“same”`. Si se selecciona `\"same\"`, entonces Keras automáticamente se encarga de todos los hiper-parámetros para que el padding sea lo suficientemente grande, tanto para que la imagen de entrada como la de salida sean exactamente las mismas y no se pierda ningún dato durante el proceso. Si se selecciona `“valid”`, entonces no aplica el “padding”. Al momento de pasar el filtro tomara el primer hasta el último pixel original de la imagen pero esto resultara en una imagen más corta en ancho y largo.\n",
    "\n",
    "# Capa MaxPooling2D\n",
    "\n",
    "A medida que la profundidad de la red va avanzando, tambien lo hace el número de imágenes generadas por los filtros y por lo tanto el número de imagenes a procesar ocasionara una sobrecarga computacional.\n",
    "\n",
    "Es por esto que se invento la capa de pooling, la cual nos servirá para disminuir el tamaño de las imágenes resultantes de la convolución. Esta capa utiliza un kernel con distintos metodos:\n",
    "\n",
    "* max: conserva el pixel mas alto de un grupo de pixeles capturados por el kernel\n",
    "* average: saca el promedio de un grupo de pixeles capturados por el kernel\n",
    "\n",
    "Estas operaciones generaran una operación de agrupamiento. Gracias a esta operación se reduce el ruido, mientras se conserva las características más importantes de la imagen.\n",
    "\n",
    "En resumen, mientras avancemos en las capas de la redes neuronales convolucionales, se ira aumentando el número de canales en la imagen (imágenes resultantes de convolución) y se ira disminuyendo el tamaño de las imágenes debido a la capa de pooling.\n",
    "\n",
    "Estas 2 operaciones permitirán detectar resaltar objetos específicos en una imagen, sin tener problemas de computación.\n",
    "\n",
    "# Arquitectura de la red convolusional\n",
    "\n",
    "Las múltiples imágenes de entradas que utiliza el modelo van a pasar a través de una red convolusional para extraer ciertas características como: bordes, texturas, etc.\n",
    "\n",
    "Al principio estas capas son superficiales pero al adentrarse a la profundidad de la red van a ser características mucho más complejas aprovechando el aprendizaje profundo (Deep Learning).\n",
    "\n",
    "Para analizar una imagen, la primera capa siempre debe ser convolusional, la cual agrega profundidad al input. Despues le sigue una capa de Max-Pooling. Estas suelen ir entrelazadas para reducir la complejidad, acortando el largo y ancho de las imágenes.\n",
    "\n",
    "Esto hace que el modelo genere una imagen mucho más pequeña pero mucho más profunda en su contexto (contiene informacion mas especifica).\n",
    "\n",
    "Después se le aplica una capa llamada “flatten” la cual lleva al arreglo o Tensor resultante a una única dimensión. Esto facilita que el apilamiento de las capas densas clasifique las imagenes.\n",
    "\n",
    "Ejemplo de arquitectura: https://alexlenail.me/NN-SVG/LeNet.html\n",
    "\n",
    "Calculadora para calcular la salida de una capa en CNN: https://madebyollin.github.io/convnet-calculator/\n",
    "\n",
    "# Data Augmentation\n",
    "\n",
    "Esta tecnica busca mejorar el rendimiento y la precisión de nuestro modelo al aumentar la cantidad y el estilo de los datos de entrada; dado que es difícil obtener y clasificar las imágenes para entrenar un modelo, dados los ambientes de una fotografía, la luz, el escenario, etc.\n",
    "\n",
    "La tecnica de Data augmentation toma una imagen y le hace distintas modificaciones, tales como:\n",
    "\n",
    "* Hacerle zoom\n",
    "* Desplazar la imagen de sus ejes X,Y\n",
    "* Aumentar el brillo\n",
    "* Rotar la imagen\n",
    "\n",
    "Ademas, tenemos un argumento llamado `fill mode`, el cual tiene 3 modos basicos:\n",
    "\n",
    "* Wrap: crea una tira consecutiva de la imagen para completar los espacios\n",
    "* Reflect: refleja la imagen sobre el eje y\n",
    "* Nearest: Cuando tu rotas una imagen se puede perder informacion de los pixeles, ya quedan pixeles negros. Entonces, este modo rellena los pixeles negros con los pixeles mas cercanos a estos.\n",
    "\n",
    "`Keras` tiene una funcion llamada `ImageDataGenerator`, la cual nos permite hacer Data augmentation a partir de los siguientes argumentos:\n",
    "\n",
    "* `rotation_range`: son los grados de rotacion que tendra la imagen.\n",
    "* `zoom_range`: es la fraccion de cuanto zoom va a hacer a la imagen.\n",
    "* `fill_mode`: colocas los distintos modos para completar imagenes.\n",
    "* `horizontal_flip`: muestra una imagen tipo espejo de la imagen\n",
    "* `brightness_range`: cambia el brillo de la imagen a traves de un rango de brillo, el cual se define dentro de una lista.\n",
    "* `Width & Height_shift_range`: se desplaza sobre el ancho y el alto de la imagen.\n",
    "\n",
    "A modo de sumario, Data Augmentation lo que hace es mantener la foto con sus características principales pero creando imágenes distintas.\n",
    "\n",
    "La máquina observa dos fotos “** parecidas**” pero con características similar. Esto es lo buscado y deseado al llevar a cabo un entrenamiento que pueda encontrar estas características\n",
    "\n",
    "# Early Stopping\n",
    "\n",
    "Esta es una tecnica que nos permite acabar el entrenamiento cuando el modelo deja de  mejorar en una metrica. El metodo `tf.keras.callbacks.earlyStopping()` nos indica cuanto tiempo puede esperar el mmodelo para que mejore una metrica.\n",
    "\n",
    "Los argumentos de este metodo son:\n",
    "\n",
    "* `monitor`: es la metrica que va a usar el modelo para evaluar el desempeño. \n",
    "* `patience`: es la cantidad de epocas que el modelo va a esperar para conseguir un mejor resultado en la metrica medida. Si no consigue un mejor resultado, entonces el modelo deja de entrenar.\n",
    "\n",
    "# Checkpoint\n",
    "\n",
    "El checkpoint es un metodo definido por `tf.keras.callbacks.ModelCheckpoint()` que permite guardar la epoca  que tuvo mejores resultados en el modelo. Usa los siguiente argumentos:\n",
    "\n",
    "* `filepath`: es el nombre con el cual esta version del modelo va a ser guardada en un archivo.\n",
    "* `verbose`: es la forma en que quieres ver el progreso del entrenamiento (estetica).\n",
    "* `monitor`: es la metrica que va a usar el modelo para evaluar el desempeño.\n",
    "* `save_best_only`: Es un booleano y sirve para decir que si quieres guardar la mejor version del modelo.\n",
    "\n",
    "Despues,este modelo puede ser llamado con el metodo `model.load_weights(filename)` el cual utiliza el path del archivo que contiene al modelo guardado.\n",
    "\n",
    "NOTA: Tando el early stopping como el checkpoint van dentro del argumento `callbacks` dentro de `model.fit()`. Esto se veria como: `model.fit(callbacks = chackpoint)`.\n",
    "\n",
    "# Batch Normalization\n",
    "\n",
    "Batch Normalizacion es una herramienta que facilita mucho el descenso del gradiente y optimiza las redes convolucionales.\n",
    "\n",
    "La normalización es un procedimiento llevado a cabo sobre un conjunto de datos que busca estandarizar sus valores, reducir la cantidad de números que lo compone y en una escala homogénea de datos ayudando al descenso del gradiente “para converger mucho más rápido ” cuando se ejecuta el “Backpropagation” . La normalizacion tiene las siguientes caracteristicas:\n",
    "\n",
    "* Valores pequeños: típicamente entre 0 a 1\n",
    "* Data homogénea: todos los píxeles tienen datos en el mismo rango.\n",
    "\n",
    "No obstante, el proceso de normalizacion tambien se puede llevar a cabo dentro de las capas ocultas de la red neuronal, a lo que se le conoce como “Batch Normalization”. Este proceso sigue la misma lógica matemática de cualquier normalización:\n",
    "\n",
    "1. Agrupar por lotes o “batches” dentro del mismo entrenamiento de la red neuronal, en consecuencia, obtengo la media del conjunto de entrenamiento.\n",
    "2. Despues, debemos obtener la varianza o desviación estándar.\n",
    "3. Luego, a cada lote se le resta la media y se divide entre la desviación estándar. \n",
    "\n",
    "Al hacer esto dentro de las capas ocultas, tenemos data estandarizada como resultado, lo que ayudanda a converger el algoritmo.\n",
    "\n",
    "Este proceso se crea a traves de una nueva capa neuronal, generada por `model.add(BatchNormalization())`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3337c905",
   "metadata": {
    "papermill": {
     "duration": 0.005431,
     "end_time": "2023-05-12T06:34:26.179450",
     "exception": false,
     "start_time": "2023-05-12T06:34:26.174019",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8870157b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T06:34:26.193167Z",
     "iopub.status.busy": "2023-05-12T06:34:26.192613Z",
     "iopub.status.idle": "2023-05-12T06:34:34.675935Z",
     "shell.execute_reply": "2023-05-12T06:34:34.674606Z"
    },
    "papermill": {
     "duration": 8.494049,
     "end_time": "2023-05-12T06:34:34.679214",
     "exception": false,
     "start_time": "2023-05-12T06:34:26.185165",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import clone_model\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.layers import Conv2D, Dropout, MaxPooling2D, Flatten, Dense\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0154b474",
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 0.006019,
     "end_time": "2023-05-12T06:34:34.691739",
     "exception": false,
     "start_time": "2023-05-12T06:34:34.685720",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1ra Red Convolucional\n",
    "\n",
    "## Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f84ab62c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T06:34:34.705764Z",
     "iopub.status.busy": "2023-05-12T06:34:34.705015Z",
     "iopub.status.idle": "2023-05-12T06:34:35.609599Z",
     "shell.execute_reply": "2023-05-12T06:34:35.608231Z"
    },
    "papermill": {
     "duration": 0.915042,
     "end_time": "2023-05-12T06:34:35.612790",
     "exception": false,
     "start_time": "2023-05-12T06:34:34.697748",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "29515/29515 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26421880/26421880 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "5148/5148 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4422102/4422102 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Carga los datos de entrada\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5901bc7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T06:34:35.630249Z",
     "iopub.status.busy": "2023-05-12T06:34:35.628206Z",
     "iopub.status.idle": "2023-05-12T06:34:35.637923Z",
     "shell.execute_reply": "2023-05-12T06:34:35.636530Z"
    },
    "papermill": {
     "duration": 0.020971,
     "end_time": "2023-05-12T06:34:35.640971",
     "exception": false,
     "start_time": "2023-05-12T06:34:35.620000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape de las imagenes de 28 x 28\n",
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "589807a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T06:34:35.656285Z",
     "iopub.status.busy": "2023-05-12T06:34:35.655966Z",
     "iopub.status.idle": "2023-05-12T06:34:35.932984Z",
     "shell.execute_reply": "2023-05-12T06:34:35.931729Z"
    },
    "papermill": {
     "duration": 0.287949,
     "end_time": "2023-05-12T06:34:35.936016",
     "exception": false,
     "start_time": "2023-05-12T06:34:35.648067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x786db4bf8f10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Observar imagen\n",
    "plt.imshow(train_images[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f2f54be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T06:34:35.953638Z",
     "iopub.status.busy": "2023-05-12T06:34:35.952963Z",
     "iopub.status.idle": "2023-05-12T06:34:36.042965Z",
     "shell.execute_reply": "2023-05-12T06:34:36.041607Z"
    },
    "papermill": {
     "duration": 0.10237,
     "end_time": "2023-05-12T06:34:36.046327",
     "exception": false,
     "start_time": "2023-05-12T06:34:35.943957",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transformacion en float y estandarizacion de imagenes sobre 255\n",
    "train_images = train_images.astype('float32')/255\n",
    "test_images = test_images.astype('float32')/255\n",
    "\n",
    "# Reshape de imagenes con una dimension extra para escala de grises\n",
    "train_images = train_images.reshape(train_images.shape[0],train_images.shape[1], train_images.shape[2],1)\n",
    "test_images = test_images.reshape(test_images.shape[0],test_images.shape[1], test_images.shape[2],1)\n",
    "\n",
    "# Vectorizar las categorias de labels con to_categorical(), donde pones el numero de categorias que existen\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels,10)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7d1abf",
   "metadata": {
    "papermill": {
     "duration": 0.007073,
     "end_time": "2023-05-12T06:34:36.061877",
     "exception": false,
     "start_time": "2023-05-12T06:34:36.054804",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Crear Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67e97ea0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T06:34:36.078622Z",
     "iopub.status.busy": "2023-05-12T06:34:36.078241Z",
     "iopub.status.idle": "2023-05-12T06:34:39.094787Z",
     "shell.execute_reply": "2023-05-12T06:34:39.093573Z"
    },
    "papermill": {
     "duration": 3.049436,
     "end_time": "2023-05-12T06:34:39.118814",
     "exception": false,
     "start_time": "2023-05-12T06:34:36.069378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 64)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 14, 14, 64)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 32)        8224      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 7, 7, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 7, 7, 32)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1568)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               401664    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 412,778\n",
      "Trainable params: 412,778\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Definimos modelo\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Creamos capa de convolucion\n",
    "model.add(Conv2D(filters = 64, kernel_size = 2, padding = 'same', activation = 'relu', input_shape = (28,28,1) ))\n",
    "\n",
    "#Creamos capa de max pooling y de dropout\n",
    "model.add(MaxPooling2D(pool_size = 2))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Repetimos con menos filtros y sin input_shape\n",
    "model.add(Conv2D(filters = 32, kernel_size = 2, padding = 'same', activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = 2))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Hacemos capa para hacer flatten las matrices\n",
    "model.add(Flatten())\n",
    "\n",
    "# Creamos capa densa con una capa de dropout\n",
    "model.add(Dense(256, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Creamos capa con activacion softmax para categorizar\n",
    "model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "# Hacemos summary del modelo\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2965d6c8",
   "metadata": {
    "papermill": {
     "duration": 0.010649,
     "end_time": "2023-05-12T06:34:39.140461",
     "exception": false,
     "start_time": "2023-05-12T06:34:39.129812",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Entrenamiento y evaluacion del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "516e7137",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T06:34:39.164519Z",
     "iopub.status.busy": "2023-05-12T06:34:39.163512Z",
     "iopub.status.idle": "2023-05-12T06:34:39.170959Z",
     "shell.execute_reply": "2023-05-12T06:34:39.169766Z"
    },
    "papermill": {
     "duration": 0.022551,
     "end_time": "2023-05-12T06:34:39.173904",
     "exception": false,
     "start_time": "2023-05-12T06:34:39.151353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Definimos el early stop\n",
    "early = tf.keras.callbacks.EarlyStopping(monitor= 'accuracy', patience = 1)\n",
    "\n",
    "# Definimos el checkpoint\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath= '1er_CNN.hdf5',\n",
    "                                               verbose = 1,\n",
    "                                               monitor = 'accuracy',\n",
    "                                               save_best_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3430c6bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T06:34:39.196973Z",
     "iopub.status.busy": "2023-05-12T06:34:39.196223Z",
     "iopub.status.idle": "2023-05-12T06:35:36.513970Z",
     "shell.execute_reply": "2023-05-12T06:35:36.512770Z"
    },
    "papermill": {
     "duration": 57.332534,
     "end_time": "2023-05-12T06:35:36.516888",
     "exception": false,
     "start_time": "2023-05-12T06:34:39.184354",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-12 06:34:40.793898: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential/dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 13s 5ms/step - loss: 0.5833 - accuracy: 0.7863\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.4010 - accuracy: 0.8574\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.3603 - accuracy: 0.8708\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.3370 - accuracy: 0.8788\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.3235 - accuracy: 0.8845\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.3123 - accuracy: 0.8877\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.3067 - accuracy: 0.8904\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.3042 - accuracy: 0.8915\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.3029 - accuracy: 0.8939\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.2964 - accuracy: 0.8956\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x786db4ae9810>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compilamos con la funcion de perdida, el optimizador y la metrica de eficiencia \n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "             optimizer = 'rmsprop',\n",
    "             metrics = ['accuracy'])\n",
    "\n",
    "# Entrenamiento con early stop\n",
    "model.fit(train_images, \n",
    "          train_labels,\n",
    "          callbacks= early ,\n",
    "          batch_size= 64,\n",
    "          epochs= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11096467",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T06:35:36.698347Z",
     "iopub.status.busy": "2023-05-12T06:35:36.697587Z",
     "iopub.status.idle": "2023-05-12T06:36:26.188020Z",
     "shell.execute_reply": "2023-05-12T06:36:26.186856Z"
    },
    "papermill": {
     "duration": 49.582786,
     "end_time": "2023-05-12T06:36:26.190999",
     "exception": false,
     "start_time": "2023-05-12T06:35:36.608213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-12 06:35:37.876165: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential/dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "932/938 [============================>.] - ETA: 0s - loss: 0.2964 - accuracy: 0.8960\n",
      "Epoch 1: accuracy improved from -inf to 0.89608, saving model to 1er_CNN.hdf5\n",
      "938/938 [==============================] - 6s 5ms/step - loss: 0.2964 - accuracy: 0.8961\n",
      "Epoch 2/10\n",
      "937/938 [============================>.] - ETA: 0s - loss: 0.2936 - accuracy: 0.8961\n",
      "Epoch 2: accuracy did not improve from 0.89608\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.2938 - accuracy: 0.8961\n",
      "Epoch 3/10\n",
      "934/938 [============================>.] - ETA: 0s - loss: 0.2917 - accuracy: 0.8967\n",
      "Epoch 3: accuracy improved from 0.89608 to 0.89667, saving model to 1er_CNN.hdf5\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.2918 - accuracy: 0.8967\n",
      "Epoch 4/10\n",
      "929/938 [============================>.] - ETA: 0s - loss: 0.2897 - accuracy: 0.8981\n",
      "Epoch 4: accuracy improved from 0.89667 to 0.89815, saving model to 1er_CNN.hdf5\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.2895 - accuracy: 0.8982\n",
      "Epoch 5/10\n",
      "933/938 [============================>.] - ETA: 0s - loss: 0.2851 - accuracy: 0.8997\n",
      "Epoch 5: accuracy improved from 0.89815 to 0.89982, saving model to 1er_CNN.hdf5\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.2851 - accuracy: 0.8998\n",
      "Epoch 6/10\n",
      "937/938 [============================>.] - ETA: 0s - loss: 0.2866 - accuracy: 0.9002\n",
      "Epoch 6: accuracy improved from 0.89982 to 0.90022, saving model to 1er_CNN.hdf5\n",
      "938/938 [==============================] - 5s 6ms/step - loss: 0.2866 - accuracy: 0.9002\n",
      "Epoch 7/10\n",
      "934/938 [============================>.] - ETA: 0s - loss: 0.2869 - accuracy: 0.8995\n",
      "Epoch 7: accuracy did not improve from 0.90022\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.2867 - accuracy: 0.8996\n",
      "Epoch 8/10\n",
      "931/938 [============================>.] - ETA: 0s - loss: 0.2830 - accuracy: 0.9014\n",
      "Epoch 8: accuracy improved from 0.90022 to 0.90147, saving model to 1er_CNN.hdf5\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.2826 - accuracy: 0.9015\n",
      "Epoch 9/10\n",
      "930/938 [============================>.] - ETA: 0s - loss: 0.2837 - accuracy: 0.9010\n",
      "Epoch 9: accuracy did not improve from 0.90147\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.2836 - accuracy: 0.9010\n",
      "Epoch 10/10\n",
      "932/938 [============================>.] - ETA: 0s - loss: 0.2830 - accuracy: 0.9006\n",
      "Epoch 10: accuracy did not improve from 0.90147\n",
      "938/938 [==============================] - 5s 5ms/step - loss: 0.2832 - accuracy: 0.9004\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x786d906cdc50>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compilamos con la funcion de perdida, el optimizador y la metrica de eficiencia \n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "             optimizer = 'rmsprop',\n",
    "             metrics = ['accuracy'])\n",
    "\n",
    "# Entrenamiento con early stop\n",
    "model.fit(train_images, \n",
    "          train_labels,\n",
    "          callbacks= checkpoint ,\n",
    "          batch_size= 64,\n",
    "          epochs= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15267bd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-12T06:36:26.536874Z",
     "iopub.status.busy": "2023-05-12T06:36:26.536429Z",
     "iopub.status.idle": "2023-05-12T06:36:28.159104Z",
     "shell.execute_reply": "2023-05-12T06:36:28.157857Z"
    },
    "papermill": {
     "duration": 1.799621,
     "end_time": "2023-05-12T06:36:28.161870",
     "exception": false,
     "start_time": "2023-05-12T06:36:26.362249",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2581 - accuracy: 0.9071\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2581315338611603, 0.9071000218391418]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar modelo y \n",
    "m1 = model\n",
    "m1.load_weights('./1er_CNN.hdf5') \n",
    "m1.evaluate(test_images,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2bfd53",
   "metadata": {
    "papermill": {
     "duration": 0.174807,
     "end_time": "2023-05-12T06:36:28.509665",
     "exception": false,
     "start_time": "2023-05-12T06:36:28.334858",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 137.744176,
   "end_time": "2023-05-12T06:36:32.413574",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-05-12T06:34:14.669398",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
